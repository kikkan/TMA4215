{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L√∏sningsforslag - Prosjekt 2: Dyp L√¶ring\n",
    "\n",
    "B√•de organiseringen av koden og omfanget av unders√∏kelsene av resultatene i denne LFen ligger p√• et litt h√∏yere niv√• enn det som er forventet av besvarelsene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettverket\n",
    "\n",
    "F√∏rst implementerer vi selve nettverket. Klassen __Model__ inneholder parametrene som skal tilpasses samt metodene som skal til for √• bruke nettverket p√• nye data etter at det er trent opp. Dette er alts√• alt som m√• lagres p√• harddisken for √• kunne bruke nettverket senere. __Model__ svarer omtrent til kapittel 2.1 i oppgaveteksten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Model():\n",
    "    '''This class contains the parameters of a model.\n",
    "    The variables W, b, w and Œº are optimized by training.\n",
    "    \n",
    "    The class also contains the activation functions and methods for forward propagation,\n",
    "    i.e. everything needed to use the model after it has been trained.\n",
    "    '''\n",
    "    def __init__(self, K, d, h):\n",
    "        '''Allocate memory and set metaparameters.\n",
    "        \n",
    "        The parameters W, b, w, Œº are initialized at random.\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        scaling_factor = np.sqrt(d)\n",
    "        self.W  = np.random.randn(K,d,d) / scaling_factor\n",
    "        self.b  = np.random.randn(K,d,1) / scaling_factor\n",
    "        self.w  = np.random.randn(d,1) / scaling_factor\n",
    "        self.Œº = np.random.randn(1) / scaling_factor\n",
    "\n",
    "    def forward(self, Y):\n",
    "        '''Run the network in the forward direction, while storing data for backwards propagation.\n",
    "        \n",
    "        Y[0,:,:] is an I by d matrix of training data. The rest of Y is memory that will\n",
    "        be used to store intermediate results needed by the back propagation.'''\n",
    "        for k in range(self.K):\n",
    "            Y[k+1,:,:] = Y[k,:,:] + self.h * self.œÉ(self.W[k,:,:] @ Y[k,:,:] + self.b[k,:])\n",
    "        Z = self.ùúÇ(np.transpose(Y[self.K,:,:]) @ self.w + self.Œº)\n",
    "        return Z, Y\n",
    "\n",
    "    def fast_forward(self, Y0):\n",
    "        '''Run the network without preparing for back propagation.\n",
    "        \n",
    "        Y0 is a I by d matrix of I inputs of size d.'''\n",
    "        Y_old = np.empty_like(Y0)\n",
    "        Y_new = Y0.copy()\n",
    "        for k in range(self.K):\n",
    "            Y_old, Y_new = Y_new, Y_old\n",
    "            Y_new[:,:] = Y_old + self.h * self.œÉ(self.W[k,:,:] @ Y_old + self.b[k,:])\n",
    "        Z = self.ùúÇ(np.transpose(Y_new) @ self.w + self.Œº)\n",
    "        return Z\n",
    "\n",
    "    def fast_landscape(self, Y):\n",
    "        '''Used to visualize how the trained net separates points in the plane.'''\n",
    "        Z = self.ùúÇ(np.transpose(Y) @ self.w + self.Œº)\n",
    "        return Z\n",
    "\n",
    "    @staticmethod\n",
    "    def œÉ(x):\n",
    "        '''Activation function.\n",
    "        \n",
    "        To use another function, inherit this class and define a new one.\n",
    "        Remember to also update dœÉ!'''\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def dœÉ(x):\n",
    "        '''Derivative of œÉ(x).'''\n",
    "        return 1.0 - np.tanh(x) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def ùúÇ(x):\n",
    "        '''Used instead of œÉ in the last step.'''\n",
    "        return (1.0 + np.tanh(x / 2.0)) / 2.0\n",
    "\n",
    "    @staticmethod\n",
    "    def dùúÇ(x):\n",
    "        '''Derivative of ùúÇ(x).'''\n",
    "        return 0.25 * (1.0 - np.tanh(x / 2.0) ** 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det neste vi gj√∏r er √• lage en egen klasse for dataene i gradienten. Dette gj√∏r det mulig √• unng√• √• allokere minne til dette i hvert skritt av treningen senere. Alle metodene er hjelpemetoder som s√∏rger for at Adam descent-algoritmen kan skrives relativt konsist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradient():\n",
    "    '''A boring class used to \"lump\" the gradient data together in one variable.\n",
    "    \n",
    "    Also good for speed, as the memory is preallocated.'''\n",
    "    def __init__(self, K, d):\n",
    "        '''\n",
    "        K: Number of layers\n",
    "        d: width of each layer.'''\n",
    "        self.W = np.zeros((K,d,d))\n",
    "        self.b = np.zeros((K,d,1))\n",
    "        self.w = np.zeros((d,1))\n",
    "        self.Œº = np.zeros((1,))\n",
    "    \n",
    "    # The following methods are only needed for Adam descent.\n",
    "    def squared(self):\n",
    "        'Inplace'\n",
    "        np.square(self.W, out=self.W)\n",
    "        np.square(self.b, out=self.b)\n",
    "        np.square(self.w, out=self.w)\n",
    "        np.square(self.Œº, out=self.Œº)\n",
    "\n",
    "    \n",
    "    def sqrt(self):\n",
    "        'Inplace'\n",
    "        np.sqrt(self.W, out=self.W)\n",
    "        np.sqrt(self.b, out=self.b)\n",
    "        np.sqrt(self.w, out=self.w)\n",
    "        np.sqrt(self.Œº, out=self.Œº)\n",
    "    \n",
    "    # The following methods are magic, just like __init__().\n",
    "    # They are used for what is called \"operator overloading\".\n",
    "    def __iadd__(self, other):\n",
    "        # Inplace add. Two cases: float and grad.\n",
    "        if isinstance(other, Gradient):\n",
    "            self.W += other.W\n",
    "            self.b += other.b\n",
    "            self.w += other.w\n",
    "            self.Œº += other.Œº\n",
    "        else:\n",
    "            self.W += other\n",
    "            self.b += other\n",
    "            self.w += other\n",
    "            self.Œº += other\n",
    "        return self\n",
    "    \n",
    "    def __imul__(self, other):\n",
    "        # Inplace multiplication. Other is float.\n",
    "        self.W *= other\n",
    "        self.b *= other\n",
    "        self.w *= other\n",
    "        self.Œº *= other\n",
    "        return self\n",
    "\n",
    "    def __itruediv__(self, other):\n",
    "        # Inplace division. Other is Gradient.\n",
    "        self.W /= other.W\n",
    "        self.b /= other.b\n",
    "        self.w /= other.w\n",
    "        self.Œº /= other.Œº\n",
    "        return self\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        # Other is float.\n",
    "        K, d, _ = self.b.shape\n",
    "        result = Gradient(K, d)\n",
    "        result.W[:] = other * self.W\n",
    "        result.b[:] = other * self.b\n",
    "        result.w[:] = other * self.w\n",
    "        result.Œº[:] = other * self.Œº\n",
    "        return result\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        # Other is float.\n",
    "        K, d, _ = self.b.shape\n",
    "        result = Gradient(K, d)\n",
    "        result.W[:] = self.W / other\n",
    "        result.b[:] = self.b / other\n",
    "        result.w[:] = self.w / other\n",
    "        result.Œº[:] = self.Œº / other\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klassen __BackPropagator__ implementerer formlene i avsnitt 2.4 i oppgaveteksten. Klassen brukes ogs√• til √• unng√• √• allokere minne til visse variabler p√• nytt i hver iterasjon av treningen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackPropagator():\n",
    "    '''This class contains some pre-allocated memory,\n",
    "    and methods doing the backwards propagation and calculation of the gradient.'''\n",
    "    def __init__(self, K, d, batch_size):\n",
    "        '''Allocationg memory.\n",
    "        \n",
    "        batch_size: The number of data points used to calculate the gradient. May be\n",
    "        I or some smaller number. See \"stochastic gradient descent\" and \"mini-batch\".\n",
    "        '''\n",
    "        self.gradient = Gradient(K, d)\n",
    "        self.P = np.empty((K+1,d,batch_size))\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def compute_gradient(self, ZmC, Y, model):\n",
    "        # Split into two functions for convenience.\n",
    "        self._backwards_propagation(ZmC, Y, model)\n",
    "        self._forward_computations(ZmC, Y, model)\n",
    "        return self.gradient\n",
    "        \n",
    "    def _backwards_propagation(self, ZmC, Y, model):\n",
    "        # The underscore before the name indicates that this method is 'private'.\n",
    "        \n",
    "        # Set right boundary value for backwards propagation.\n",
    "        self.P[-1,:,:] = np.outer(model.w,\n",
    "                            np.multiply(ZmC,\n",
    "                                        model.dùúÇ(np.transpose(Y[-1,:,:]) @ model.w + model.Œº)\n",
    "                                       )\n",
    "                           )\n",
    "        # Backwards propagation.\n",
    "        for k in range(model.K,0,-1):\n",
    "            self.P[k-1,:,:] = self.P[k,:,:] +\\\n",
    "                model.h * np.transpose(model.W[k-1,:,:]) @ np.multiply(\n",
    "                  model.dœÉ(model.W[k-1,:,:] @ Y[k-1,:,:] + model.b[k-1,:]),\n",
    "                  self.P[k,:,:])\n",
    "\n",
    "    def _forward_computations(self, ZmC, Y, model):\n",
    "        # Compute gradient of projection parameters w and Œº.\n",
    "        aa  = model.dùúÇ(np.transpose(Y[-1,:,:]) @ model.w + model.Œº)\n",
    "        self.gradient.Œº[:] = np.sum(ZmC * aa)\n",
    "        self.gradient.w[:,:] = Y[-1,:,:] @ np.multiply(ZmC,aa)\n",
    "        # Sweep through all layers for gradient calculation.\n",
    "        for k in range(model.K):\n",
    "            aa = model.h * np.multiply(\n",
    "                self.P[k+1,:,:], model.dœÉ(model.W[k,:,:] @ Y[k,:,:] + model.b[k,:]))\n",
    "            self.gradient.W[k,:,:] = aa @ np.transpose(Y[k,:,:])\n",
    "            self.gradient.b[k,:,:] = aa @ np.ones((self.batch_size,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trening\n",
    "I dette kapittelet implementerer vi selve treningen av det nevrale nettverket. Dette tilsvarer avsnitt 3.1 i oppgaveteksten. Deretter introduserer vi de mer sofistikerte treningsalgoritmene beskrevet i avsnitt 2.3 i oppgaveteksten.\n",
    "\n",
    "Siden vi √∏nsker √• kunne kj√∏re trening med alle kombinasjoner av plain vanilla gradient, Adams, og stochastic gradient descent, skriver vi f√∏rst √©n klasse som beskriver den overordnede gangen i l√¶ringen. Deretter implementerer vi subklasser som legger til de ulike konkrete metodene.\n",
    "\n",
    "*P.S.*\n",
    "\n",
    "Legg merke til at alle parametrene til __init__()-metodene, med unntak av __model__, ikke er en del av selve modellen. N√•r nettverket f√∏rst er trenkt, er det ikke behov for disse. B√•de treningsdata og optimeringsalgoritmer er alts√• ting man kan holde hemmelig for brukerne av nettverket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractOptimizer():\n",
    "    def __init__(self, model, training_data, training_labels, maxiter, **kwargs):\n",
    "        '''Takes in the model (the neural net itself), the training set, and some metaparameters.'''\n",
    "        self.model = model\n",
    "        self.training_data = training_data\n",
    "        self.training_labels = training_labels\n",
    "        self.maxiter = maxiter\n",
    "        self.d, self.training_set_size = training_data.shape\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        '''This method does the heavy lifting.\n",
    "        \n",
    "        NB! Can take some time.'''\n",
    "        converged = False\n",
    "        it = 0\n",
    "        while not converged:\n",
    "            it += 1\n",
    "            self._iterate()\n",
    "            converged = (it > self.maxiter)\n",
    "\n",
    "    def _iterate(self):\n",
    "        self.select_batch()\n",
    "        Z, Y = self.model.forward(self.Y)    # First run a forward sweep to compute the classifier Z.\n",
    "        ZmC = Z - self.C\n",
    "        self._testing(ZmC)\n",
    "        gradient = self.bp.compute_gradient(ZmC, Y, self.model)\n",
    "        self._update_model(gradient)\n",
    "        \n",
    "    def _testing(self, ZmC_training):\n",
    "        pass\n",
    "\n",
    "    def _allocate_variables(self):\n",
    "        self.C = np.empty((self.batch_size,1), dtype='bool')\n",
    "        self.Y = np.empty((self.model.K+1,self.d,self.batch_size))\n",
    "        self.bp = BackPropagator(self.model.K, self.d, self.batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det er selvf√∏lgelig en fare for over-tilpasning, s√• kryssvalidering er viktig. F√∏lgende kode gj√∏r det mulig √• holde √∏ye med hvordan det g√•r i l√∏pet av treningen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingOptimizer(AbstractOptimizer):\n",
    "    '''Class that stores some metircs during the optimization.'''\n",
    "    def __init__(self, model, training_data, training_labels, maxiter, *, testing_data, testing_labels, **kwargs):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, **kwargs)\n",
    "        self.testing_data = testing_data\n",
    "        self.testing_labels = testing_labels\n",
    "        self.testing_set_size = testing_data.shape[1]\n",
    "        self.testing_residuals = []\n",
    "        self.error_rates = []\n",
    "        self.residuals = []\n",
    "\n",
    "    def _testing(self, ZmC_training):\n",
    "        # Residual of objective function\n",
    "        residual = 0.5 / self.batch_size * np.linalg.norm(ZmC_training)**2\n",
    "        self.residuals.append(residual)\n",
    "        \n",
    "        Z = self.model.fast_forward(self.testing_data)\n",
    "        ZmC = Z - self.testing_labels\n",
    "        residual = 0.5 / self.testing_set_size * np.linalg.norm(ZmC)**2\n",
    "        self.testing_residuals.append(residual)\n",
    "        \n",
    "        failures = np.mean(np.abs(np.round(ZmC)))\n",
    "        self.error_rates.append(failures)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_convergence(op):\n",
    "    plt.semilogy(np.array(op.residuals), label='Training residual')\n",
    "    plt.semilogy(np.array(op.testing_residuals), label='Testing residual')\n",
    "    plt.semilogy(np.array(op.error_rates), label='Training error rate')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F√∏rst implementerer vi den enkleste optimeringen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonBatchOptimizer(AbstractOptimizer):\n",
    "    '''This class runs the learning. Tweaking the learning method is done by subclassing this class.'''\n",
    "    def __init__(self, model, training_data, training_labels, maxiter, **kwargs):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, **kwargs)\n",
    "        self.batch_size = self.training_set_size\n",
    "        self._allocate_variables()\n",
    "        self.C[:] = self.training_labels\n",
    "        self.Y[0,:,:] = self.training_data\n",
    "\n",
    "    def select_batch(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class PlainGradientOptimizer(AbstractOptimizer):\n",
    "    def __init__(self, model, training_data, training_labels, maxiter, *, ùúè, **kwargs):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, **kwargs)\n",
    "        self.ùúè = ùúè\n",
    "\n",
    "    def _update_model(self, gradient):\n",
    "        '''Update the model, using plain vanilla gradient descent.'''\n",
    "        self.model.W -= self.ùúè * gradient.W\n",
    "        self.model.b -= self.ùúè * gradient.b\n",
    "        self.model.w -= self.ùúè * gradient.w\n",
    "        self.model.Œº -= self.ùúè * gradient.Œº\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N√•r koden over fungerer, kan vi fors√∏ke √• implementere de mer sofistikerte algoritmene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchOptimizer(AbstractOptimizer):\n",
    "    def __init__(self, model, training_data, training_labels, maxiter, *, batch_size, **kwargs):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, **kwargs)\n",
    "        self.batch_size = batch_size\n",
    "        self._allocate_variables()\n",
    "\n",
    "    def select_batch(self):\n",
    "        i = np.random.randint(self.training_set_size - self.batch_size + 1)\n",
    "        self.Y[0,:,:] = self.training_data[:, i:i + self.batch_size]\n",
    "        self.C[:] = self.training_labels[i:i + self.batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer(AbstractOptimizer):\n",
    "    def __init__(self, model, training_data, training_labels, maxiter, **kwargs):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, **kwargs)\n",
    "        self.m = Gradient(self.model.K, self.d)\n",
    "        self.v = Gradient(self.model.K, self.d)\n",
    "        self.j = 1\n",
    "        \n",
    "    def _update_model(self, gradient):\n",
    "        '''Actual Adam descent method.\n",
    "        \n",
    "        Much of the work is done by operator overloading in the class Gradient.'''\n",
    "        Œ≤1 = 0.9\n",
    "        Œ≤2 = 0.999\n",
    "        Œ± = 0.01\n",
    "        Œµ = 1e-8\n",
    "        #\n",
    "        self.m *= Œ≤1\n",
    "        self.m += (1 - Œ≤1) * gradient\n",
    "        m = self.m / (1 - Œ≤1**self.j)\n",
    "        \n",
    "        gradient.squared()\n",
    "        self.v *= Œ≤2\n",
    "        self.v += (1 - Œ≤2) * gradient\n",
    "        v = self.v / (1 - Œ≤2**self.j)\n",
    "        v.sqrt()\n",
    "        v += Œµ\n",
    "        \n",
    "        m /= v\n",
    "        m *= Œ±\n",
    "\n",
    "        self.model.W -= m.W\n",
    "        self.model.b -= m.b\n",
    "        self.model.w -= m.w\n",
    "        self.model.Œº -= m.Œº\n",
    "\n",
    "        self.j += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Og ved hjelp av magien som kalles \"multiple inheritance\", har vi plutselig tilgang p√• alle kombinasjonene av algoritmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest case.\n",
    "class PlainNonBatch(PlainGradientOptimizer, NonBatchOptimizer, LoggingOptimizer):\n",
    "    def __init__(self, model, training_data, training_labels, testing_data, testing_labels, maxiter, ùúè):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, ùúè=ùúè, testing_data=testing_data, testing_labels=testing_labels)\n",
    "\n",
    "# Add the Adam descent algorithm.\n",
    "class AdamNonBatch(AdamOptimizer, NonBatchOptimizer, LoggingOptimizer):\n",
    "    def __init__(self, model, training_data, training_labels, testing_data, testing_labels, maxiter):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, testing_data=testing_data, testing_labels=testing_labels)\n",
    "\n",
    "# Add mini-batches instead.\n",
    "class PlainBatch(PlainGradientOptimizer, BatchOptimizer, LoggingOptimizer):\n",
    "    def __init__(self, model, training_data, training_labels, testing_data, testing_labels, maxiter, ùúè, batch_size):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, ùúè=ùúè, batch_size=batch_size, testing_data=testing_data, testing_labels=testing_labels)\n",
    "\n",
    "# Add both. IT JUST WORKS‚Ñ¢.\n",
    "class AdamBatch(AdamOptimizer, BatchOptimizer, LoggingOptimizer):\n",
    "    def __init__(self, model, training_data, training_labels, testing_data, testing_labels, maxiter, batch_size):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, batch_size=batch_size, testing_data=testing_data, testing_labels=testing_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
