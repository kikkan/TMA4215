{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Løsningsforslag - Prosjekt 2: Dyp Læring\n",
    "\n",
    "Både organiseringen av koden og omfanget av undersøkelsene av resultatene i denne LFen ligger på et litt høyere nivå enn det som er forventet av besvarelsene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettverket\n",
    "\n",
    "Først implementerer vi selve nettverket. Klassen __Model__ inneholder parametrene som skal tilpasses samt metodene som skal til for å bruke nettverket på nye data etter at det er trent opp. Dette er altså alt som må lagres på harddisken for å kunne bruke nettverket senere. __Model__ svarer omtrent til kapittel 2.1 i oppgaveteksten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Model():\n",
    "    '''This class contains the parameters of a model.\n",
    "    The variables W, b, w and μ are optimized by training.\n",
    "    \n",
    "    The class also contains the activation functions and methods for forward propagation,\n",
    "    i.e. everything needed to use the model after it has been trained.\n",
    "    '''\n",
    "    def __init__(self, K, d, h):\n",
    "        '''Allocate memory and set metaparameters.\n",
    "        \n",
    "        The parameters W, b, w, μ are initialized at random.\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        scaling_factor = np.sqrt(d)\n",
    "        self.W  = np.random.randn(K,d,d) / scaling_factor\n",
    "        self.b  = np.random.randn(K,d,1) / scaling_factor\n",
    "        self.w  = np.random.randn(d,1) / scaling_factor\n",
    "        self.μ = np.random.randn(1) / scaling_factor\n",
    "\n",
    "    def forward(self, Y):\n",
    "        '''Run the network in the forward direction, while storing data for backwards propagation.\n",
    "        \n",
    "        Y[0,:,:] is an I by d matrix of training data. The rest of Y is memory that will\n",
    "        be used to store intermediate results needed by the back propagation.'''\n",
    "        for k in range(self.K):\n",
    "            Y[k+1,:,:] = Y[k,:,:] + self.h * self.σ(self.W[k,:,:] @ Y[k,:,:] + self.b[k,:])\n",
    "        Z = self.𝜂(np.transpose(Y[self.K,:,:]) @ self.w + self.μ)\n",
    "        return Z, Y\n",
    "\n",
    "    def fast_forward(self, Y0):\n",
    "        '''Run the network without preparing for back propagation.\n",
    "        \n",
    "        Y0 is a I by d matrix of I inputs of size d.'''\n",
    "        Y_old = np.empty_like(Y0)\n",
    "        Y_new = Y0.copy()\n",
    "        for k in range(self.K):\n",
    "            Y_old, Y_new = Y_new, Y_old\n",
    "            Y_new[:,:] = Y_old + self.h * self.σ(self.W[k,:,:] @ Y_old + self.b[k,:])\n",
    "        Z = self.𝜂(np.transpose(Y_new) @ self.w + self.μ)\n",
    "        return Z\n",
    "\n",
    "    def fast_landscape(self, Y):\n",
    "        '''Used to visualize how the trained net separates points in the plane.'''\n",
    "        Z = self.𝜂(np.transpose(Y) @ self.w + self.μ)\n",
    "        return Z\n",
    "\n",
    "    @staticmethod\n",
    "    def σ(x):\n",
    "        '''Activation function.\n",
    "        \n",
    "        To use another function, inherit this class and define a new one.\n",
    "        Remember to also update dσ!'''\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def dσ(x):\n",
    "        '''Derivative of σ(x).'''\n",
    "        return 1.0 - np.tanh(x) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def 𝜂(x):\n",
    "        '''Used instead of σ in the last step.'''\n",
    "        return (1.0 + np.tanh(x / 2.0)) / 2.0\n",
    "\n",
    "    @staticmethod\n",
    "    def d𝜂(x):\n",
    "        '''Derivative of 𝜂(x).'''\n",
    "        return 0.25 * (1.0 - np.tanh(x / 2.0) ** 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det neste vi gjør er å lage en egen klasse for dataene i gradienten. Dette gjør det mulig å unngå å allokere minne til dette i hvert skritt av treningen senere. Alle metodene er hjelpemetoder som sørger for at Adam descent-algoritmen kan skrives relativt konsist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradient():\n",
    "    '''A boring class used to \"lump\" the gradient data together in one variable.\n",
    "    \n",
    "    Also good for speed, as the memory is preallocated.'''\n",
    "    def __init__(self, K, d):\n",
    "        '''\n",
    "        K: Number of layers\n",
    "        d: width of each layer.'''\n",
    "        self.W = np.zeros((K,d,d))\n",
    "        self.b = np.zeros((K,d,1))\n",
    "        self.w = np.zeros((d,1))\n",
    "        self.μ = np.zeros((1,))\n",
    "    \n",
    "    # The following methods are only needed for Adam descent.\n",
    "    def squared(self):\n",
    "        'Inplace'\n",
    "        np.square(self.W, out=self.W)\n",
    "        np.square(self.b, out=self.b)\n",
    "        np.square(self.w, out=self.w)\n",
    "        np.square(self.μ, out=self.μ)\n",
    "\n",
    "    \n",
    "    def sqrt(self):\n",
    "        'Inplace'\n",
    "        np.sqrt(self.W, out=self.W)\n",
    "        np.sqrt(self.b, out=self.b)\n",
    "        np.sqrt(self.w, out=self.w)\n",
    "        np.sqrt(self.μ, out=self.μ)\n",
    "    \n",
    "    # The following methods are magic, just like __init__().\n",
    "    # They are used for what is called \"operator overloading\".\n",
    "    def __iadd__(self, other):\n",
    "        # Inplace add. Two cases: float and grad.\n",
    "        if isinstance(other, Gradient):\n",
    "            self.W += other.W\n",
    "            self.b += other.b\n",
    "            self.w += other.w\n",
    "            self.μ += other.μ\n",
    "        else:\n",
    "            self.W += other\n",
    "            self.b += other\n",
    "            self.w += other\n",
    "            self.μ += other\n",
    "        return self\n",
    "    \n",
    "    def __imul__(self, other):\n",
    "        # Inplace multiplication. Other is float.\n",
    "        self.W *= other\n",
    "        self.b *= other\n",
    "        self.w *= other\n",
    "        self.μ *= other\n",
    "        return self\n",
    "\n",
    "    def __itruediv__(self, other):\n",
    "        # Inplace division. Other is Gradient.\n",
    "        self.W /= other.W\n",
    "        self.b /= other.b\n",
    "        self.w /= other.w\n",
    "        self.μ /= other.μ\n",
    "        return self\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        # Other is float.\n",
    "        K, d, _ = self.b.shape\n",
    "        result = Gradient(K, d)\n",
    "        result.W[:] = other * self.W\n",
    "        result.b[:] = other * self.b\n",
    "        result.w[:] = other * self.w\n",
    "        result.μ[:] = other * self.μ\n",
    "        return result\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        # Other is float.\n",
    "        K, d, _ = self.b.shape\n",
    "        result = Gradient(K, d)\n",
    "        result.W[:] = self.W / other\n",
    "        result.b[:] = self.b / other\n",
    "        result.w[:] = self.w / other\n",
    "        result.μ[:] = self.μ / other\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klassen __BackPropagator__ implementerer formlene i avsnitt 2.4 i oppgaveteksten. Klassen brukes også til å unngå å allokere minne til visse variabler på nytt i hver iterasjon av treningen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackPropagator():\n",
    "    '''This class contains some pre-allocated memory,\n",
    "    and methods doing the backwards propagation and calculation of the gradient.'''\n",
    "    def __init__(self, K, d, batch_size):\n",
    "        '''Allocationg memory.\n",
    "        \n",
    "        batch_size: The number of data points used to calculate the gradient. May be\n",
    "        I or some smaller number. See \"stochastic gradient descent\" and \"mini-batch\".\n",
    "        '''\n",
    "        self.gradient = Gradient(K, d)\n",
    "        self.P = np.empty((K+1,d,batch_size))\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def compute_gradient(self, ZmC, Y, model):\n",
    "        # Split into two functions for convenience.\n",
    "        self._backwards_propagation(ZmC, Y, model)\n",
    "        self._forward_computations(ZmC, Y, model)\n",
    "        return self.gradient\n",
    "        \n",
    "    def _backwards_propagation(self, ZmC, Y, model):\n",
    "        # The underscore before the name indicates that this method is 'private'.\n",
    "        \n",
    "        # Set right boundary value for backwards propagation.\n",
    "        self.P[-1,:,:] = np.outer(model.w,\n",
    "                            np.multiply(ZmC,\n",
    "                                        model.d𝜂(np.transpose(Y[-1,:,:]) @ model.w + model.μ)\n",
    "                                       )\n",
    "                           )\n",
    "        # Backwards propagation.\n",
    "        for k in range(model.K,0,-1):\n",
    "            self.P[k-1,:,:] = self.P[k,:,:] +\\\n",
    "                model.h * np.transpose(model.W[k-1,:,:]) @ np.multiply(\n",
    "                  model.dσ(model.W[k-1,:,:] @ Y[k-1,:,:] + model.b[k-1,:]),\n",
    "                  self.P[k,:,:])\n",
    "\n",
    "    def _forward_computations(self, ZmC, Y, model):\n",
    "        # Compute gradient of projection parameters w and μ.\n",
    "        aa  = model.d𝜂(np.transpose(Y[-1,:,:]) @ model.w + model.μ)\n",
    "        self.gradient.μ[:] = np.sum(ZmC * aa)\n",
    "        self.gradient.w[:,:] = Y[-1,:,:] @ np.multiply(ZmC,aa)\n",
    "        # Sweep through all layers for gradient calculation.\n",
    "        for k in range(model.K):\n",
    "            aa = model.h * np.multiply(\n",
    "                self.P[k+1,:,:], model.dσ(model.W[k,:,:] @ Y[k,:,:] + model.b[k,:]))\n",
    "            self.gradient.W[k,:,:] = aa @ np.transpose(Y[k,:,:])\n",
    "            self.gradient.b[k,:,:] = aa @ np.ones((self.batch_size,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trening\n",
    "I dette kapittelet implementerer vi selve treningen av det nevrale nettverket. Dette tilsvarer avsnitt 3.1 i oppgaveteksten. Deretter introduserer vi de mer sofistikerte treningsalgoritmene beskrevet i avsnitt 2.3 i oppgaveteksten.\n",
    "\n",
    "Siden vi ønsker å kunne kjøre trening med alle kombinasjoner av plain vanilla gradient, Adams, og stochastic gradient descent, skriver vi først én klasse som beskriver den overordnede gangen i læringen. Deretter implementerer vi subklasser som legger til de ulike konkrete metodene.\n",
    "\n",
    "*P.S.*\n",
    "\n",
    "Legg merke til at alle parametrene til __init__()-metodene, med unntak av __model__, ikke er en del av selve modellen. Når nettverket først er trenkt, er det ikke behov for disse. Både treningsdata og optimeringsalgoritmer er altså ting man kan holde hemmelig for brukerne av nettverket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractOptimizer():\n",
    "    def __init__(self, model, training_data, training_labels, maxiter, **kwargs):\n",
    "        '''Takes in the model (the neural net itself), the training set, and some metaparameters.'''\n",
    "        self.model = model\n",
    "        self.training_data = training_data\n",
    "        self.training_labels = training_labels\n",
    "        self.maxiter = maxiter\n",
    "        self.d, self.training_set_size = training_data.shape\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        '''This method does the heavy lifting.\n",
    "        \n",
    "        NB! Can take some time.'''\n",
    "        converged = False\n",
    "        it = 0\n",
    "        while not converged:\n",
    "            it += 1\n",
    "            self._iterate()\n",
    "            converged = (it > self.maxiter)\n",
    "\n",
    "    def _iterate(self):\n",
    "        self.select_batch()\n",
    "        Z, Y = self.model.forward(self.Y)    # First run a forward sweep to compute the classifier Z.\n",
    "        ZmC = Z - self.C\n",
    "        self._testing(ZmC)\n",
    "        gradient = self.bp.compute_gradient(ZmC, Y, self.model)\n",
    "        self._update_model(gradient)\n",
    "        \n",
    "    def _testing(self, ZmC_training):\n",
    "        pass\n",
    "\n",
    "    def _allocate_variables(self):\n",
    "        self.C = np.empty((self.batch_size,1), dtype='bool')\n",
    "        self.Y = np.empty((self.model.K+1,self.d,self.batch_size))\n",
    "        self.bp = BackPropagator(self.model.K, self.d, self.batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det er selvfølgelig en fare for over-tilpasning, så kryssvalidering er viktig. Følgende kode gjør det mulig å holde øye med hvordan det går i løpet av treningen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingOptimizer(AbstractOptimizer):\n",
    "    '''Class that stores some metircs during the optimization.'''\n",
    "    def __init__(self, model, training_data, training_labels, maxiter, *, testing_data, testing_labels, **kwargs):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, **kwargs)\n",
    "        self.testing_data = testing_data\n",
    "        self.testing_labels = testing_labels\n",
    "        self.testing_set_size = testing_data.shape[1]\n",
    "        self.testing_residuals = []\n",
    "        self.error_rates = []\n",
    "        self.residuals = []\n",
    "\n",
    "    def _testing(self, ZmC_training):\n",
    "        # Residual of objective function\n",
    "        residual = 0.5 / self.batch_size * np.linalg.norm(ZmC_training)**2\n",
    "        self.residuals.append(residual)\n",
    "        \n",
    "        Z = self.model.fast_forward(self.testing_data)\n",
    "        ZmC = Z - self.testing_labels\n",
    "        residual = 0.5 / self.testing_set_size * np.linalg.norm(ZmC)**2\n",
    "        self.testing_residuals.append(residual)\n",
    "        \n",
    "        failures = np.mean(np.abs(np.round(ZmC)))\n",
    "        self.error_rates.append(failures)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_convergence(op):\n",
    "    plt.semilogy(np.array(op.residuals), label='Training residual')\n",
    "    plt.semilogy(np.array(op.testing_residuals), label='Testing residual')\n",
    "    plt.semilogy(np.array(op.error_rates), label='Training error rate')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Først implementerer vi den enkleste optimeringen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonBatchOptimizer(AbstractOptimizer):\n",
    "    '''This class runs the learning. Tweaking the learning method is done by subclassing this class.'''\n",
    "    def __init__(self, model, training_data, training_labels, maxiter, **kwargs):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, **kwargs)\n",
    "        self.batch_size = self.training_set_size\n",
    "        self._allocate_variables()\n",
    "        self.C[:] = self.training_labels\n",
    "        self.Y[0,:,:] = self.training_data\n",
    "\n",
    "    def select_batch(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class PlainGradientOptimizer(AbstractOptimizer):\n",
    "    def __init__(self, model, training_data, training_labels, maxiter, *, 𝜏, **kwargs):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, **kwargs)\n",
    "        self.𝜏 = 𝜏\n",
    "\n",
    "    def _update_model(self, gradient):\n",
    "        '''Update the model, using plain vanilla gradient descent.'''\n",
    "        self.model.W -= self.𝜏 * gradient.W\n",
    "        self.model.b -= self.𝜏 * gradient.b\n",
    "        self.model.w -= self.𝜏 * gradient.w\n",
    "        self.model.μ -= self.𝜏 * gradient.μ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når koden over fungerer, kan vi forsøke å implementere de mer sofistikerte algoritmene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchOptimizer(AbstractOptimizer):\n",
    "    def __init__(self, model, training_data, training_labels, maxiter, *, batch_size, **kwargs):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, **kwargs)\n",
    "        self.batch_size = batch_size\n",
    "        self._allocate_variables()\n",
    "\n",
    "    def select_batch(self):\n",
    "        i = np.random.randint(self.training_set_size - self.batch_size + 1)\n",
    "        self.Y[0,:,:] = self.training_data[:, i:i + self.batch_size]\n",
    "        self.C[:] = self.training_labels[i:i + self.batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer(AbstractOptimizer):\n",
    "    def __init__(self, model, training_data, training_labels, maxiter, **kwargs):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, **kwargs)\n",
    "        self.m = Gradient(self.model.K, self.d)\n",
    "        self.v = Gradient(self.model.K, self.d)\n",
    "        self.j = 1\n",
    "        \n",
    "    def _update_model(self, gradient):\n",
    "        '''Actual Adam descent method.\n",
    "        \n",
    "        Much of the work is done by operator overloading in the class Gradient.'''\n",
    "        β1 = 0.9\n",
    "        β2 = 0.999\n",
    "        α = 0.01\n",
    "        ε = 1e-8\n",
    "        #\n",
    "        self.m *= β1\n",
    "        self.m += (1 - β1) * gradient\n",
    "        m = self.m / (1 - β1**self.j)\n",
    "        \n",
    "        gradient.squared()\n",
    "        self.v *= β2\n",
    "        self.v += (1 - β2) * gradient\n",
    "        v = self.v / (1 - β2**self.j)\n",
    "        v.sqrt()\n",
    "        v += ε\n",
    "        \n",
    "        m /= v\n",
    "        m *= α\n",
    "\n",
    "        self.model.W -= m.W\n",
    "        self.model.b -= m.b\n",
    "        self.model.w -= m.w\n",
    "        self.model.μ -= m.μ\n",
    "\n",
    "        self.j += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Og ved hjelp av magien som kalles \"multiple inheritance\", har vi plutselig tilgang på alle kombinasjonene av algoritmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest case.\n",
    "class PlainNonBatch(PlainGradientOptimizer, NonBatchOptimizer, LoggingOptimizer):\n",
    "    def __init__(self, model, training_data, training_labels, testing_data, testing_labels, maxiter, 𝜏):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, 𝜏=𝜏, testing_data=testing_data, testing_labels=testing_labels)\n",
    "\n",
    "# Add the Adam descent algorithm.\n",
    "class AdamNonBatch(AdamOptimizer, NonBatchOptimizer, LoggingOptimizer):\n",
    "    def __init__(self, model, training_data, training_labels, testing_data, testing_labels, maxiter):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, testing_data=testing_data, testing_labels=testing_labels)\n",
    "\n",
    "# Add mini-batches instead.\n",
    "class PlainBatch(PlainGradientOptimizer, BatchOptimizer, LoggingOptimizer):\n",
    "    def __init__(self, model, training_data, training_labels, testing_data, testing_labels, maxiter, 𝜏, batch_size):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, 𝜏=𝜏, batch_size=batch_size, testing_data=testing_data, testing_labels=testing_labels)\n",
    "\n",
    "# Add both. IT JUST WORKS™.\n",
    "class AdamBatch(AdamOptimizer, BatchOptimizer, LoggingOptimizer):\n",
    "    def __init__(self, model, training_data, training_labels, testing_data, testing_labels, maxiter, batch_size):\n",
    "        super().__init__(model, training_data, training_labels, maxiter, batch_size=batch_size, testing_data=testing_data, testing_labels=testing_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
